% \documentclass[rnd]{mas_proposal}
\documentclass[thesis]{mas_proposal}

\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{float}
\usepackage{array}
\usepackage{hyperref}

\newcolumntype{C}[1]{>{\centering\arraybackslash}p{#1}}
\newcommand{\widthsym}{1.6cm}
\newcommand{\widthdesc}{7cm}
\newcommand{\widthdel}{5cm}
\newcommand{\widthdescdel}{12cm}

\renewcommand{\arraystretch}{1.5}
\newcommand{\cmark}{\ding{51}}%
\newcommand{\xmark}{\ding{55}}%

\title{Deep Learning-Based Adaptation of Robot Behaviour for Assistive Robotics}
\author{Michał Stolarz}
\supervisors{Prof. Dr. Paul G. Plöger\\MSc. Alex Mitrevski}
\date{January 2023}

\thirdpartylogo{images/migrave.png}

\begin{document}

\maketitle

\pagestyle{plain}

\section[Introduction]{Introduction\footnote{Parts of this chapter have been published in~\cite{stolarz2022personalized,stolarz2022learningbased}}}
In this section, we introduce the motivation and goal of the master's thesis, as well as explain various terms related to adaptation techniques used in the Human-Robot Interaction (HRI) field.
\subsection{Motivation}
One of the objectives of robot-assisted therapy (RAT)~\cite{esteban2017build} is increasing the autonomy of the robot that is used during therapy sessions; this has the purpose of reducing the necessary therapist interactions with the robot, while still keeping the therapist in control of the sessions at all times.
In the context of RAT, robot programs are usually developed in such a way that they can be used generically for different individuals; however, individuals may have different reactions to specific stimuli and, depending on their concrete needs, may also benefit from therapy sessions focusing on specific aspects.
This means that a generic RAT approach may not be optimal for the effective treatment of individuals; instead, the robot should be able to adapt its behaviour to the needs of each individual and therapy session \cite{esteban2017build,scassellati2018improving,rudovic2018personalized}.

The motivation behind this work is therapy for children with Autism Spectrum Disorder (ASD). 
This problem is of significant relevance as in the European Union, there are over 5 million people affected by autism~\cite{deenigma2022} and it is estimated that 1 in 160 children all over the world is diagnosed with ASD~\cite{jain2020modeling}. 
People with ASD often have difficulties in social interaction and communication. 
To alleviate the effects of ASD, individualised therapies are provided. 
However, autistic children find robots easier to communicate with than humans~\cite{robins2006}, thus Robot-Assisted Therapies (RATs) have been investigated. During RAT, most of the time therapists have to control the robot remotely (Wizard of Oz approach)~\cite{deenigma2022}~\cite{david2018developing,robins2017developing,rudovic2017measuring,marinoiu20183d}. 
Because of it, the therapist might not be able to fully focus on the therapy and react appropriately to the child's behaviour~\cite{cao2018personalized}. 
To reduce their workload, the autonomy of the robot has to be increased, namely, it should be able to interpret a child’s behaviour and adapt its actions to the individual needs of the child~\cite{esteban2017build}.

\subsection{Adaptation Techniques}
Adaptation is possible if the robot actively learns a user model that encodes certain attributes of the user. The user model can be integrated into a robot decision-making algorithm~\cite{rossi2017user} called a behaviour model, which allows the system to choose appropriate robot reactions in response to the actions of each individual user. Personalisation refers to the adaptation of the system to the individual user over time~\cite{rossi2017user} and can be solved by using Interactive Machine Learning (IML), which involves the user in the learning loop~\cite{senft2019teaching}. IML usually makes use of \emph{learning from guidance} or \emph{learning from feedback}. Learning from guidance relies on an external supervisor (e.g. therapist), who provides expert knowledge to the system. The supervisor is able to assess the decisions of the robot before being executed, namely, they are able to accept, or alternatively reject and override the suggested reaction of the robot. This solution guarantees that the system will not execute any undesirable actions during learning, but is sensitive to the mistakes of the supervising person. On the other hand, learning from feedback uses direct feedback from the user (e.g. engagement level of the user). As there is no supervising person, the robot has to explore by itself what effects its actions have. 

\subsection{Project Goal}
\label{subsec:project_goal}
The main problem during RAT for children with autism is that the therapists have to control the robot manually, which might meaningfully increase their workload. This means that there is a need for a personalised behaviour model which will increase the autonomy of the robot. The model should interpret and continuously adapt to the behaviour of the individual child under therapy, as each child might have different ASD symptoms. The therapy for children with ASD usually consists of learning activities designed by the therapists. This means that the developed learning algorithm should enable the robot to personalise the difficulty of the learning activities to the individual child's skill level. Additionally, the robot should also react appropriately when interacting with the child does not go as planned. That means that the robot should prevent them from getting bored, disengaged or demotivated, by executing actions such as giving verbal motivating feedback or simple motions (e.g. waving gesture) that would draw the child’s attention back to the activity.

Currently, in order to enable the robot to react appropriately in various social situations during an interaction with the user, many works make use of an engagement estimator. Engagement is the feature that is often used for the development of behaviour models~\cite{senft2015sparc,senft2015human,tsiakas2018task,del2022learning}. It can be measured with the use of EEG headset~\cite{tsiakas2018task}, but an external engagement observer might be more convenient for children with ASD, as they may be overwhelmed by the sensory stimuli if they need to wear an additional device during therapy~\cite{javed2019robotic}. In the literature, several types of algorithms that estimate engagement from features obtained using the OpenFace library~\cite{baltrusaitis2018openface, jain2020modeling, kaur2019domain, karimah2021implementation}, eye gaze~\cite{khorrami2014system},  body posture~\cite{ritschel2017adapting} or visual data~\cite{mane2018engagement, del2020you} can be found. Some of these are also able to capture and classify temporal data~\cite{del2020you, karimah2021implementation}.

However, the model estimating an engagement used further for the decision-making process is imperfect and might introduce an additional error to the behaviour model. This is a problem with a significant impact on the robot behaviour and was mentioned in our recent work~\cite{stolarz2022learningbased}. To alleviate the effect of false predictions on the feature level we suggest turning towards data-driven methods that will be able to use raw sensor data instead of high-level features (e.g. engagement level) that have to be estimated separately. However, the tabular approaches like Q-learning (used in our recent work~\cite{stolarz2022learningbased}) are unsuitable for big state spaces~\cite{akalin2021reinforcement}, like raw sensory data. That is why in this work, we are planning to use deep learning (DL) for creating a visuomotor behaviour model. In the next section, we introduce the deficits of the related works and the DL approach proposed by us.

\section{Problem Statement}
\label{sec:problem_statement}

The problem that is going to be approached during the project is developing a personalised behaviour model which will increase the autonomy of the social robot during RAT for children with ASD. This should decrease the workload of the therapist that will not have to control the robot remotely. The model should be able to perform both social behaviour and activity difficulty personalisation (both terms are introduced in Section~\ref{sec:related_work}), as the children should increase their skills and stay engaged during the intervention.

As we want to avoid an error provided by an additional model, such as an engagement estimator (calculating high-level features for the behaviour model), we want to develop a decision-making algorithm that operates on the raw sensory data. As the conventional tabular approaches are unsuitable for that purpose~\cite{akalin2021reinforcement}, we focus on the DL techniques.

\subsection{Deficits to Be Solved}
%First of all, the DL neural networks require a big amount of data in order to converge, which is why a big number of interactions with the user is needed. Moreover, the training is computationally heavy, which is why it can not be performed continuously during HRI~\cite{Qureshi2016,Qureshi2017,Qureshi2018,Romeo2019}. The developed models for social behaviour adaptation in many cases were trained and evaluated on the previously collected datasets~\cite{ClarkTurner2017,Romeo2018,Hijaz2021} or in the simulation~\cite{Turner2018,Belo2021,Belo2022}. Moreover, the data is often collected in the laboratory environment (instead of the real one), which might not prepare the model for the real-life interaction~\cite{ClarkTurner2017,Turner2018,Romeo2018}.

Based on our literature search, in HRI there are no DL-based solutions for both social behaviour and activity difficulty adaptation. In the field of science related to games, there are only works describing applications of DRL algorithms as virtual~\cite{mnih2015human,hausknecht2015deep,sorokin2015deep} or robotic~\cite{Cuayahuitl2017,Cuayahuitl2020} opponents. Moreover, none of the existing approaches for social behaviour adaptation are used for personalisation (to the individual user) nor conducting learning activities with a user. When it comes to the existing approaches there are two main deficits to be solved. The performance of the known solutions is not acceptable for the clinical intervention, due to the high possibility of making a mistake by the robot~\cite{ClarkTurner2017,Turner2018,Romeo2019,Hijaz2021}. This might be caused by the insufficient number of interactions with the user (not enough data) or the fact that the collected data is imbalanced (some actions are performed more often than others)~\cite{Hijaz2021}. Moreover, many of the known DL social behaviour adaptation solutions have an inappropriate action space for conducting a RAT~\cite{Qureshi2016,Qureshi2017,Qureshi2018,ClarkTurner2017,Turner2018,Belo2021,Belo2022,Romeo2018,Romeo2019}.

In this project we plan to face the following deficits:
\begin{itemize}
	\item enabling the robot to personalise the activity difficulty and social behaviour, by:
	\begin{itemize} 
	\item providing the DL network not only with sensory data but also with an extra input vector (similarly to~\cite{Qureshi2018,Belo2022,Hijaz2021}) containing the learning activity information,
	\item maintenance of an adequate action space enabling the robot to give feedback to the user and change a difficulty of an activity, similarly to~\cite{stolarz2022learningbased,stolarz2022personalisedrobot,tsiakas2018task},
	\end{itemize}
	\item preventing the model from making socially unacceptable decisions and thus making it suitable for the clinical application, by pretraining on the manually collected dataset and applying learning from the guidance approach, similarly to~\cite{senft2017supervised}.
\end{itemize}

\subsection{Proposed Approach}
\label{subsec:proposed_approach}

Our proposed approach would be an adaptation of SocialDQN~\cite{Belo2022}. Instead of an extra input (additional to the input with 8 grayscale images) in the form of one-hot vector encoding for the social signals, we suggest an extra input that would encode the current activity state, as depicted in Fig.~\ref{fig:proposed_architecture}. This solution would provide the network with the necessary information for giving engaging feedback to the user (based on the image), as well as adapting the difficulty of the activity (based on the game signals). The game signals vector would contain information about the last chosen difficulty level and if the user succeeded in solving the task.

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.9\textwidth]{images/architecture/proposed_architecture1.png}
	\caption{Adapted architecture of SocialDQN~\cite{Belo2022}, where the green boxes indicate the suggested changes that constitute our proposed approach}
	\label{fig:proposed_architecture}
\end{figure}

The difference between our approach and~\cite{Belo2022} is also the action space, which will consist of 5 actions. Similarly, as in~\cite{tsiakas2018task,stolarz2022learningbased,stolarz2022personalisedrobot}, the actions space will enable the robot to (i) set one of the three difficulty levels and (ii) provide an encouraging or challenging feedback. Regarding the reward for the model, we suggest a similar approach as in~\cite{senft2017supervised}, which is learning from guidance. We make an assumption that during the therapy for children with ASD, there is always a therapist that can accept or overwrite the actions of the robot (before being executed). We thus base the reward on the information from the therapist, who is a supervisor of the system in that case. The feedback from the therapist will not only influence the reward but also the action executed in the end. We believe, that similar changes (introducing an extra input vector with game signals, increasing the number of possible actions, retraining based on the feedback from the supervisor) can be done to the CNN model presented in~\cite{Romeo2018,Romeo2019}.

\subsection{Evaluation}
We will evaluate the proposed approach (i) on the data from the study conducted in~\cite{stolarz2022learningbased,stolarz2022personalisedrobot} and (ii) during a real-life evaluation as in~\cite{Qureshi2016,romeo2021human}. In order to increase the amount of training data we will apply augmentation techniques (e.g. mirroring the image in the vertical axis as mentioned in~\cite{romeo2021human}). It is important to mention, that we will evaluate the model's ability for personalisation (not only adaptation) only if we manage to obtain (collect and augment) enough data samples for each individual user or user groups.~\footnote{We refer to the user groups described in~\cite{stolarz2022personalisedrobot}}

\section[Related Work]{Related Work\footnote{Parts of this chapter have been published in~\cite{stolarz2022personalized}}}
\label{sec:related_work}

As we discussed in~\cite{stolarz2022personalized}, there are four personalisation approaches that are provided in the context of HRI, namely: social behaviour, game difficulty, affective, and user preferences (e.g. proxemics) personalisation. However, the main focus should be put on two first personalisation techniques. The DL has been explored in the field of social behaviour learning and game difficulty adaptation. However, the former is only about adaptation, not personalisation, as social behaviour was learned based on interaction with many users. On the other hand, the latter refers more to the agent not as a tutor but as an opponent in the game. In this section, we introduce the most relevant approaches belonging to both adaptation categories, as well as the available datasets and tools for manual data collection.

\subsection{Deep Learning Adaptation Approaches}

\subsubsection{Social Behaviour Learning}
Qureshi et al.~\cite{Qureshi2016} made a step towards making robots more interactive while coexisting with humans. This is done by enabling the robot to continuously learn social interaction skills. For that purpose, the deep reinforcement learning (DRL) method was used, which is called Multimodal Deep Q-Network (MDQN)\footnote{The original implementation is available under \url{https://github.com/ahq1993/Multimodal-Deep-Q-Network-for-Social-Human-Robot-Interaction}}\footnote{The python implementation (used in~\cite{Belo2021}) is available under \url{https://github.com/JPedroRBelo/pyMDQN}} and is based on DQN~\cite{mnih2015human}. This is a major contribution, as the developed DRL interaction model can conduct HRI in an uncontrolled and real environment, which is not the case for the previous works. There are two inputs for MDQN, namely grayscale and depth video frames. Based on these two streams the model learns when to perform one of the four actions, which are: \emph{wait}, \emph{look towards human}, \emph{wave hand}, \emph{handshake}. The reward function is designed such that the robot's main goal is to perform a successful handshake. Here the data generation and training phases are separated, which means that all the robot's experiences are saved in the replay memory and used for training only during the resting period. 

In~\cite{Qureshi2017} the MDQN model was augmented by adding a recurrent attention model~\cite{sorokin2015deep} in order to enable the network to focus on certain fragments of the input data. This approach was proven to increase people's willingness to interact with a robot (through handshaking). However, the new model requires more training data in order to reach the performance of the neural network without attention. There are three deficits of the aforementioned approaches which are related to deploying the robot in the real environment. Firstly, the action space consists only of four actions, which might be insufficient for reacting to human behaviour in real life. Secondly, the designed reward function is task-specific and may be difficult to adjust to different applications (here handshaking is rewarded explicitly); it should be mentioned as well that external rewards are scarce and might make learning relatively slow. Moreover, as training the neural network is computationally heavy, it has to be performed during the robot's resting period, as otherwise, it could cause delays during the interaction period. This is a significant disadvantage which may cause a user to be disengaged, as the policy is not updated continuously during the interaction. 

In the follow-up work~\cite{Qureshi2018}, the authors faced the second deficit and proposed an intrinsically motivated DRL. The internal reward, used for training the Deep Q-network (Fig.~\ref{fig:mdqn}), is calculated with the use of an action-conditional prediction network (Pnet), as an error between Pnet's event prediction and detected event occurrence. The aforementioned events are an observed behaviour of the interacting partner, such as a handshake, eye contact and smile. It is moreover shown that intrinsically motivated DRL leads to more human-like behaviour than DRL based on an external reward. However, during the experiments, it was noticed that the robot was repeating a handshake action even after the already successful handshake. This behaviour may be unusual from the perspective of the normal person and should be avoided (e.g. with memory that would prevent the robot from being oblivious).

\begin{figure}[htb!]
	\centering
	\includegraphics[width=0.7\textwidth]{images/architecture/mdqn.png}
	\caption{Architecture of the Deep Q-Network~\cite{Qureshi2018}}
	\label{fig:mdqn}
\end{figure}

Belo et al.~\cite{Belo2022} also made an approach to make robots more socially acceptable and natural while interacting with humans. For that purpose, the Social Robotics Deep Q-Network (SocialDQN) was developed, which is based on the MDQN architecture. However, in SocialDQN the input for the depth information was substituted with the input indicating the emotional state of the user with Ekman's six basic emotions~\cite{ekman1971constants}: \emph{fear}, \emph{happiness}, \emph{disgust}, \emph{anger}, \emph{sadness}, \emph{surprise}. The authors conducted tests in the simulator~\cite{Belo2021} and asked 13 human referees to assess if the robot performed correct actions in each of the considered scenarios. In the end, three models were compared, namely a random policy action selector, and SocialDQN with and without social signals as an extra input. It was shown that SocialDQN (that considers social signals) outperforms the two other predictors. It reached an accuracy of 89.50\%. Unfortunately, no comparative study was performed with an original MDQN. Moreover, the evaluation was performed only in the simulation and not during a real-life interaction with a real human. Another deficit is that the action space of SocialDQN is relatively small as it consists of only four actions (similar to MDQN).
 
Clark-Turner et al.~\cite{ClarkTurner2017} aim at developing a framework enabling the robot to deliver a behavioural intervention (BI). The idea behind BI is to teach children with certain disorders to perform new behaviours. The authors used for that purpose deep recurrent Q-network (DRQN) which is trained from human demonstrations. This is a major contribution as no previous work used DRQN to learn a human policy for choosing discrete actions based on the demonstration data. There are two inputs for the developed model, namely: RGB image, point cloud and an audio spectrogram. The set of possible actions consists of \emph{command}, \emph{prompt} (in both cases the robot waves and greets the user, with the exception that the latter provides also a prompt saying what is expected from the participant), \emph{reward} (giving a positive appraisal to the user followed by ending the session by saying goodbye), \emph{abort} (ending the session when the user's response is negative consequently). The reward for the robot is positive whenever the actions are performed correctly, otherwise, no reward is provided. Unfortunately, no real-life tests were conducted as the proposed algorithm was evaluated only with the collected demonstration data, where the robot was teleoperated. Moreover, the obtained results show a relatively small accuracy of the model (43.2\%-83.3\%) which may be insufficient for deploying it for real behavioural intervention.

Both of the aforementioned deficits were faced in the next work of the same authors~\cite{Turner2018}. First of all, the approach to increase a model's accuracy with transfer learning was made. It means that the network extracting features from the RGB data was an IRNV2 network pre-trained on the ImageNet. Additionally, the used modalities were changed, namely, a point cloud was replaced with an optical flow image. Unfortunately, no comparative evaluation between an old and new approach was performed, so the improvement of the introduced changes can not be directly shown. However, the authors faced the second deficit and performed not only evaluation on the collected dataset but also real-life tests. The reported accuracy in both cases was not exceeding 80\%. Moreover, the authors limited the number of actions to three, namely: \emph{PMT}, \emph{REW}, \emph{END} (which correspond to \emph{prompt}, \emph{reward} and \emph{abort} respectively).

In~\cite{Romeo2018}, the goal of the authors is to develop a robot that would accompany the elderly. For that purpose, the authors developed a multimodal system that can make a decision on how and with whom it should start an interaction. There are two contributions of this work, namely, the collection of the dataset, focusing on the initiation of the interaction, and training a CNN on it. The data was collected in two sessions, during the first one single users were interacting with the robot and during the second one two participants were acting in front of the robot. The aforementioned CNN was trained and evaluated on the collected dataset. The input of the network consists of the grayscale images and the output of three actions: \emph{waiting}, \emph{calling for attention} (waving and introducing), \emph{starting the interaction} (asking a direct question). The reward was designed to be positive or zero in case of the correct use of the action and negative otherwise. The deficit of that work is that the trained CNN was evaluated only on the dataset (not in the real-life interaction) and the limited action space (only three actions). However, the reported accuracy is promising as it reached 93\%. 

In the next work~\cite{Romeo2019}, Romeo et al. faced one deficit of~\cite{Romeo2018} and evaluated the model during real-life tests that lasted for four days. To improve the model's capabilities, at the end of each day, the network was fully retrained (including the newly collected data). According to the reported results, the accuracy of the network, obtained on the updated dataset was still 93\% at the end of the last experimentation day. However, the accuracy of the CNN, obtained during an actual interaction with the study participants and calculated as a number of correctly chosen actions accordingly to the participants' opinions, was 44\%, 58\%, 56\% and 59\% (each one obtained on every single day). The additional details of the conducted experiments and the model can be found in~\cite{romeo2021human}.

Hijaz et al. directly approached the problem of increasing the autonomy of the robot used during the therapies for individuals with ASD~\cite{Hijaz2021}. The authors propose a system for learning the therapist's verbal behaviour during the intervention delivery through Learning from Demonstration (LfD). There are two contributions to the aforementioned work. Firstly, in other works the data used for training the model is collected in a simulation~\cite{Turner2018,Belo2021,Belo2022} or laboratory environment~\cite{ClarkTurner2017,Turner2018,Romeo2018} which usually simplifies real-world interaction. In this work, however, the data was collected in a clinical setting, during a real intervention, where the robot was teleoperated by a therapist. Secondly, in the previous works, the actions delivered by the robot were pre-scripted which is the cause of the learned behaviour being unnatural in the end. In~\cite{Hijaz2021} the actions were taken directly from the demonstrations given by therapists (as the robot was teleoperated during the intervention). The input to the used deep neural network (DNN) model was a child audio spectrogram and last therapist behaviour. The action space is relatively big in comparison to other works~\cite{Qureshi2016,Qureshi2017,Qureshi2018,ClarkTurner2017,Turner2018,Belo2021,Belo2022,Romeo2018,Romeo2019} as it consists of 9 actions, namely: saying that the robot is in one of six emotional states (angry, surprised, tired, scared, happy, sad), discriminative stimulus (asking the child how the robot is feeling followed by emotion presentation using body language), social praise (in case the child gives a correct answer during a game) and random actions (used to maintain child's engagement). The main deficit of the presented work is a very low accuracy (43.48\%), which might be due to the insufficient amount of collected data, that was reported as imbalanced. Moreover, the system was not tested in a real-life scenario, but only on the collected dataset. 

\subsubsection{Game Difficulty Adaptation}
When it comes to game difficulty adaptation, the DL algorithms were used so far as opponent players in the board/computer games. In~\cite{Silver2016} the authors managed to develop the algorithm (based on the deep neural network) that was able to defeat a professional player in the game of Go. The DQN was proved to achieve the performance of professional game testers~\cite{mnih2015human}. In~\cite{hausknecht2015deep} the authors added Long Short-Term Memory (LSTM)~\cite{hochreiter1997long} layer in order to enable DQN to remember distant events. In~\cite{sorokin2015deep}, the authors managed to improve the performance of DQN by adding an attention mechanism, which also increased the interpretability of the network decisions (ability to determine on which regions of the image the network is focusing). 

Finally, in the robotics field, in~\cite{Cuayahuitl2017}, the author designed another variant of DQN which was further deployed on the conversational robot playing the \emph{Tic-tac-toe} game. The main aim of the work was to develop a humanoid robot that would play games with people having a brain illness, and thus slow down their decline. The deployed model is an improvement to the previous work as it provides not only game difficulty but also social behaviour adaptation. The robot is able to play the game with the user, as well as perform certain dialogue acts. In total, the action space is relatively big as it provides 18-34 actions (depending on the size of \emph{Tic-tac-toe} grid), consisting of dialogue acts (speech and arm movements) and game moves. The state space consists of the features describing the game moves and words that occurred during an interaction. The authors evaluated the system with the user simulation (with semi-random behaviour). The results indicate that the system was able to conduct successful interactions and the proposed variant of DQN obtains higher winning rates than the original DQN.

The deficit of~\cite{Cuayahuitl2017} was faced in~\cite{Cuayahuitl2020} where the authors performed a real-world evaluation. The proposed variant of DQN (in~\cite{Cuayahuitl2020} extended to two variants of \emph{Tic-tac-toe} game) was tested with 130 study participants for four nonconsecutive days in-the-wild (outside of the laboratory environment). The authors reported that the users were impressed with the robot, however, no quantitative or qualitative measures were provided.

However, all of the aforementioned works describe a DRL agent as an opponent playing a game against a human user. No solutions for DRL just choosing the game difficulty for the user were found. Akalin et al.~\cite{Akalin2018} proposed an initial idea for a DRL for adjusting the difficulty level of the game during a social human-robot interaction with elderly people. According to the authors, the input would consist of three variables, namely, the user's valence and engagement, as well as the state of the game (the last difficulty level and information if the game is stopped or paused). The action space would consist of five actions: increasing, decreasing, not changing a difficulty level or pausing or stopping the game. The reward would be calculated based on the valence and engagement of the user. Unfortunately,~\cite{Akalin2018} is just a proposal of the solution without any implementation details or results. Additionally, the input and reward for the algorithm consists of high-level features, which (as explained in Section~\ref{subsec:project_goal}) might introduce additional error to the behaviour model.

\subsection{Available Datasets}
 For training DL agents a huge amount of data is required, which explains the necessity for a dataset. We looked for publicly available datasets that can be used for training DL decision-making agents. Unfortunately, all of them can only be applied for testing the feasibility of social behaviour learning models. Because the datasets contain visual data we categorised them into two groups, depending on the perspective of the camera from which the video/pictures were recorded. The following datasets contain data recorded from the first-person perspective (the human partner/s is/are directly interacting with the robot):
\begin{itemize}
	\item AIR-Act2Act~\cite{Ko2021}: 100 subjects, 10 actions, 5000 samples, 3 modalities (depth, skeleton, RGB)
	\item NTU RGB+D~\cite{Shahroudy_2016_CVPR,Liu2020}: 106 subjects, 26 actions, 8276 samples, 3 modalities (depth, skeleton, RGB).
	\item \cite{ryoo2013firstperson}: 8 subjects, 7 actions, 684 samples, 1 modality (RGB) 
	\item \cite{ryoo2015robot}: 8 subjects, 9 actions, 180 samples, 2 modalities (RGB, depth)
	\item AUTH UAV Gesture~\cite{patrona2021overview} (contains parts of datasets presented in \cite{Shahroudy_2016_CVPR,Perera_2018_ECCV_Workshops}): 8 subjects (newly captured), 6 actions, 4930 samples, 1 modality (RGB) 
\end{itemize}

Most of the aforementioned datasets are designed for activity recognition~\cite{Shahroudy_2016_CVPR,Liu2020,ryoo2013firstperson}, activity prediction~\cite{ryoo2015robot} and gesture recognition~\cite{patrona2021overview}. Only the AIR-Act2Act~\cite{Ko2021} dataset was designed with the purpose of teaching the robot social skills, namely, the scenario, where the study participant is performing an action, is labelled with the reaction type that the robot should undertake (Fig.~\ref{fig:act2act_sample}). Additionally, this dataset contains the robot behaviours (body gestures) that should be performed as a reaction. None of the aforementioned datasets can be used for training the model that would be applicable to ASD therapy. However, they can be used for checking the feasibility of the DL architecture (e.g. MDQN) that can be further trained in the simulation or on the manually collected dataset. Unfortunately, only certain datasets are easily available online, namely: AIR-Act2Act, AUTH UAV Gesture, and NTU RGB+D.

\begin{figure}[htb!]
	\centering
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{images/dataset/1.png}
		\subcaption{}%
		\label{subfig:seq1}
	\end{subfigure}
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{images/dataset/2.png}
		\subcaption{}%
		\label{subfig:seq2}
	\end{subfigure}
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{images/dataset/3.png}
		\subcaption{}%
		\label{subfig:seq3}
	\end{subfigure}
	\begin{subfigure}[b]{0.22\textwidth}
		\centering
		\includegraphics[width=0.9\textwidth]{images/dataset/4.png}
		\subcaption{}%
		\label{subfig:seq4}
	\end{subfigure}
	\caption{Frames (preprocessed) from one of the videos from AIR-Act2Act dataset~\cite{Ko2021} (the video depicts a scenario where the robot is supposed to bow to the elderly person who enters the apartment)}
	\label{fig:act2act_sample}
\end{figure}

The requirement for using a certain dataset is recordings from the first-person perspective (the person participating in the interaction). That is why the datasets containing only recordings from the perspective of the third person (two people are interacting with each other and the robot is an observer)~\cite{Hu2013,UT-Interaction-Data,Gemeren2016,Yun2012} are unsuitable for our application.

\subsection{Tools for Data Collection}

As mentioned before, the provided datasets are not suitable for training the model that would be applicable to ASD therapy. That is why during the project it will be required to collect the dataset manually or in the simulation. In both cases the data will not be collected from the individuals with autism, however, the scenarios imitating a robot-conducted intervention can be replicated with people without autism (students and university staff) or with simulated users. For both cases, there are available tools that can be reused in the proposed project.

First of all, the autism intervention scenarios can be simulated with the Simulator for Deep Reinforcement Learning and Social Robotics (SimDRLSR)~\cite{Belo2021}.\footnote{\url{https://github.com/JPedroRBelo/simDRLSR}} Here, the actions of the simulated humans are defined with scripts. Additionally, the robot's actions can be evaluated (whether they are socially acceptable) by human referees with a provided validation tool~\cite{Belo2022}\footnote{\url{https://github.com/JPedroRBelo/validation_tool_socialdqn}}. Although a simulation is a cheap and fast method for data collection for training a DL agent, the important features for action selection during an autism intervention are facial expressions, eye gaze and head orientation of the users~\cite{stolarz2022learningbased}, which might not be possible to reflect in the simulation.

Another possibility is to collect the data manually. This requires suitable software that would enable a researcher to control the robot, as well as appropriately organise and store the collected data. There are publicly available packages that can be used as a guideline for creating such software~\cite{Turner2018,carpio2018learning,carpio2019learning}.\footnote{\url{https://github.com/AssistiveRoboticsUNH/deep_reinforcement_abstract_lfd}}$^,$\footnote{\url{https://github.com/AssistiveRoboticsUNH/TR-LfD}}

\section{Project Plan}

\subsection{Work Packages}
During the master's thesis, the following working packages will be delivered:

\begin{enumerate}
	\item[WP1] \textbf{Literature search on the DL adaptation techniques used in HRI} \\ Objectives:
	\begin{itemize}
		\item Search the DL-based social behaviour learning approaches commonly used in HRI.
		\item Search the DL-based game difficulty adaptation techniques commonly used in HRI. 
	\end{itemize}
	\item [WP2] \textbf{Selection of the best performing DL architecture for a social behaviour learning} \\
	Objectives:
	\begin{itemize}
		\item Select available DL-based social behaviour learning architectures~\cite{Romeo2018,Romeo2019,Qureshi2016,Qureshi2017,Belo2022}.
		\item Select suitable datasets~\cite{Ko2021,Shahroudy_2016_CVPR,Liu2020,patrona2021overview}.
		\item Compare the previously selected DL architectures based on one of the previously selected datasets and choose the best-performing model.
	\end{itemize}
	\item [WP3] \textbf{Adaptation of the social behaviour DL model for the sequence-learning game use case} \\
	Objectives:
	\begin{itemize}
		\item Adapt the chosen DL architecture, such that it can be used for both social behaviour learning and game difficulty adaptation (the proposed approach is described in Section~\ref{subsec:proposed_approach}).
		\item Perform initial training and evaluation of the adapted architecture on the dataset collected for our previous works~\cite{stolarz2022personalisedrobot,stolarz2022learningbased}.
	\end{itemize}
	\item [WP4] \textbf{Evaluation of the learning framework during the real-life interaction} \\
	Objectives:
	\begin{itemize}
		\item Prepare the experimentation setup and gather volunteers for the experiments. 
		\item Conduct an evaluation with the proposed DL model (deployed on the QTrobot or NAO) similar to~\cite{Qureshi2016,romeo2021human}.
	\end{itemize}
	\item [WP5] \textbf{Project report} \\
	Objectives:
	\begin{itemize}
		\item Merge the results from the previous work packages.
		\item Write the final report.
	\end{itemize}
\end{enumerate}

\subsection{Milestones}
\begin{enumerate}
	\item[M1] \textbf{DL behaviour model selection} \\
	At this milestone, the literature search in the field of DL in social behaviour learning and game difficulty adaptation should be done. Based on the evaluation of the selected social behaviour models the best-performing one should be found. The respective results of the literature search and models' evaluation should be summarized in the form of reports.
	\item[M2] \textbf{DL behaviour model adaptation and experimental setup} \\
	At this milestone, the most adequate social behaviour model should be adapted to the sequence-learning game and initially evaluated with a dataset collected for our previous works. If the aforementioned dataset will not be sufficient (e.g. too small amount of samples) the model will be trained and evaluated only during the real-life evaluation, for which the experimentation setup should be prepared. At this milestone, the implementation of the required DL algorithm as well as the learning/evaluation pipeline should be ready. Additionally, the software for real-life evaluation should be prepared adequately.
	\item[M3] \textbf{Real-life evaluation and report submission} \\
	At this milestone, the results of the real-life evaluation of the DL behaviour model should have been obtained. The final master's thesis report, containing results from this and all the previous milestones, should be submitted.
\end{enumerate}

\subsection{Tasks and Deliverables}
Tasks belonging to each working package as well as related deliverables are presented in the \autoref{tab:tasks_deliverables}.
%\begin{table}[h]
{
	\footnotesize
	\begin{longtable}{ |p{\widthsym}||p{\widthdesc}|p{\widthdel}|  }
		\hline
		Working Package~/ Task&Description&Deliverable\\
		\hline
		\textbf{WP1} & \multicolumn{2}{C{\widthdescdel}|}{\textbf{Literature search on the DL adaptation techniques used in HRI}} \\
		\hline
		T1.1 & Research on DL-based social behaviour learning concepts used in HRI & D1.1: Report with the overview of the commonly used methods\\
		\hline
		T1.2 & Research on DL-based game difficulty adaptation concepts commonly used in HRI & D1.2: Report with the overview of the commonly used methods\\
		\hline
		\textbf{WP2}  & \multicolumn{2}{C{\widthdescdel}|}{\textbf{Selection of the best performing DL architecture for a social behaviour learning}} \\
		\hline
		T2.1 & Selection of the suitable DL-based social behaviour learning architectures & \multirow{2}{\linewidth}{D2.1: Report with the description of the selected architectures and dataset}\\
		\cline{1-2}
		T2.2 & Selection of the suitable datasets &
		\\
		\hline
		T2.3 & Comparative evaluation of the previously selected DL architectures based on one of the previously selected datasets and choice of the best-performing model & D2.2: Implementation of the models and report with the evaluation results\\
		\hline
		\textbf{WP3} & \multicolumn{2}{C{\widthdescdel}|}{\textbf{Adaptation of the social behaviour DL model for the sequence-learning game}} \\
		\hline
		T3.1 & Adaptation of the chosen DL architecture, such that it can be used for both social behaviour learning and game difficulty adaptation & D3.1: Implemented software \\
		\hline
		T3.2 & Initial training and evaluation of the adapted architecture on the dataset collected for our previous works & D3.2: Report with the evaluation results \\
		\hline
		\textbf{WP4} & \multicolumn{2}{C{\widthdescdel}|}{\textbf{Evaluation of the learning framework during the real-life interaction}} \\
		\hline
		T4.1 & Preparation of the experimentation setup & D4.1: Implemented software \\
		\hline
		T4.2 & Evaluation with the proposed DL model (deployed on the QTrobot or NAO) & D4.2: Evaluation report\\
		\hline
		\textbf{WP5}  & \multicolumn{2}{C{\widthdescdel}|}{\textbf{Project report}} \\
		\hline
		T5.1 & Combining the reports from the previous deliverables & \multirow{3}{\linewidth}{D5.1: Draft of the master's thesis report}\\
		\cline{1-2}
		T5.2 & Detailed description of the real-world experimentation & \\
		\cline{1-2}
		T5.3 & Creation of the proper visualisations of the evaluation data & \\
		\cline{1-3}
		T5.4 & Iterative corrections with the supervisors & D5.2: Final master's thesis report\\
		\hline
		\caption{Tasks and deliverables (WP stands for working package, T for task, D for deliverable)}
		\label{tab:tasks_deliverables}
	\end{longtable}
}
%Notes:
% correct the deficits of the related work (no small action space) and summarize all the deficits 

\subsection{Project Schedule}
The project schedule is illustrated as a Gantt chart (\autoref{fig:gantt}). It includes the tasks as well as the milestones mentioned in the previous subsections.

\begin{figure}[h!]
	\includegraphics[width=0.9\textwidth]{images/plan/plan.png}
	\caption{Project schedule}
	\label{fig:gantt}
\end{figure}

\clearpage

\bibliographystyle{plainnat} % Use the plainnat bibliography style
\bibliography{bibliography.bib} % Use the bibliography.bib file as the source of references

\end{document}
